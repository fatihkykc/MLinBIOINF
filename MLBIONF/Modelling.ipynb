{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# from scipy.stats import kruskal\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "# from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# maple_train_df = pd.read_csv('maple_train.csv')\n",
    "# maple_test_df = pd.read_csv('maple_test.csv')\n",
    "# maple_eval_df = pd.read_csv('maple_eval.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# path = './PathwayFeatures'\n",
    "# pathway_names = []\n",
    "# pw_names = maple_eval_df[\"Path\"].unique()\n",
    "# for i, attrlist in enumerate(os.listdir(path)):\n",
    "#     attributes = pd.read_csv(path+'/'+ attrlist)\n",
    "#     attributes = attributes[\"Attrib\"].tolist()\n",
    "#     pathway_name = attrlist.split('-')[0]\n",
    "#     pathway_names.append(pathway_name)\n",
    "#\n",
    "#     maple_train_df.loc[maple_train_df['Path'] == pathway_name.replace('_', ' '), 'Class'] = 'positive'\n",
    "#     maple_test_df.loc[maple_test_df['Path'] == pathway_name.replace('_', ' '), 'Class'] = 'positive'\n",
    "#     maple_eval_df.loc[maple_eval_df['Path'] == pathway_name.replace('_', ' '), 'Class'] = 'positive'\n",
    "#\n",
    "#     maple_train_df.loc[maple_train_df['Path'] != pathway_name.replace('_', ' '), 'Class'] = 'negative'\n",
    "#     maple_test_df.loc[maple_test_df['Path'] != pathway_name.replace('_', ' '), 'Class'] = 'negative'\n",
    "#     maple_eval_df.loc[maple_eval_df['Path'] != pathway_name.replace('_', ' '), 'Class'] = 'negative'\n",
    "#\n",
    "#     train_df_prep = maple_train_df[attributes]\n",
    "#     test_df_prep = maple_test_df[attributes]\n",
    "#     eval_df_prep = maple_eval_df[attributes]\n",
    "#\n",
    "#     train_df_prep.to_csv('Datasets/'+pathway_name+'_train.csv', index=False)\n",
    "#     test_df_prep.to_csv('Datasets/'+pathway_name+'_test.csv', index=False)\n",
    "#     eval_df_prep.to_csv('Datasets/'+pathway_name+'_eval.csv', index=False)\n",
    "#\n",
    "#     if (i == 1):\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n",
    "# by: @invoktheshell\n",
    "# def perf_measure(y_actual, y_hat):\n",
    "#     TP = 0\n",
    "#     FP = 0\n",
    "#     TN = 0\n",
    "#     FN = 0\n",
    "#\n",
    "#     for i in range(len(y_hat)):\n",
    "#         if y_actual[i]==y_hat[i]==1:\n",
    "#            TP += 1\n",
    "#         if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "#            FP += 1\n",
    "#         if y_actual[i]==y_hat[i]==0:\n",
    "#            TN += 1\n",
    "#         if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "#            FN += 1\n",
    "#\n",
    "#     return(TP, FP, TN, FN)\n",
    "#\n",
    "# from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef, recall_score, precision_score\n",
    "#\n",
    "# def calc_metrics(y_true, y_pred, model_name, pw_name, dataset_name, fold_num):\n",
    "#     acc = accuracy_score(y_true, y_pred)\n",
    "#     tp, fp, tn, fn = perf_measure(y_true, y_pred)\n",
    "#     recall = precision_score(y_true, y_pred)\n",
    "#     specifity = tn / (tn + fp)\n",
    "#     f1 = f1_score(y_true, y_pred)\n",
    "#     mcc = matthews_corrcoef(y_true, y_pred)\n",
    "#     precision = precision_score(y_true, y_pred)\n",
    "#     # print(precision, specifity, tp,fp,tn,fn)\n",
    "#\n",
    "#     try:\n",
    "#         fpr = fp / (fp + tn)\n",
    "#     except:\n",
    "#         fpr = 0\n",
    "#\n",
    "#     try:\n",
    "#         tpr = tp / (tp + fn)\n",
    "#     except:\n",
    "#         tpr = 0\n",
    "#\n",
    "#     try:\n",
    "#         fnr = fn / (fn + tp)\n",
    "#     except:\n",
    "#         fnr = 0\n",
    "#\n",
    "#     try:\n",
    "#         fdr = fp / (fp + tp)\n",
    "#     except:\n",
    "#         fdr = 0\n",
    "#\n",
    "#     return model_name, pw_name, dataset_name, fold_num, acc, f1, mcc, tp, fp, tn, fn, recall, precision, specifity, fpr, tpr, fnr, fdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#.3 Metabolic pathway modeling and analysis\n",
    "# The 10-fold cross-validation and bagging were used on all training process,\n",
    "# and the metrics accuracy, percentage of correctly classified instances (CCI),\n",
    "# true positive (TP) rate, false positive (FP) rate, false negative (FN) rate, recall,\n",
    "# specificity, F-measure, false discovery rate (FDR), and Matthews coefficient correlation (MCC)\n",
    "# were used to evaluate the predictive performances.\n",
    "\n",
    "\n",
    "\n",
    "# res_dict = defaultdict(list)\n",
    "# res = []\n",
    "#\n",
    "# for pathway_name in pathway_names:\n",
    "#     train_df = pd.read_csv('Datasets/'+ pathway_name+'_train.csv')\n",
    "#     # test_df = pd.read_csv('Datasets/'+ pathway_name+'_test.csv')\n",
    "#     # eval_df = pd.read_csv('Datasets/'+ pathway_name+'_eval.csv')\n",
    "#\n",
    "#     # print('pathway: ', pathway_name)\n",
    "#     # print(len(train_df[train_df[\"Class\"]=='positive']))\n",
    "#\n",
    "#     train_df[\"Class\"] = train_df[\"Class\"].apply(lambda x: 1 if x=='positive' else 0)\n",
    "#     # test_df[\"Class\"] = test_df[\"Class\"].apply(lambda x: 1 if x=='positive' else 0)\n",
    "#     # eval_df[\"Class\"] = eval_df[\"Class\"].apply(lambda x: 1 if x=='positive' else 0)\n",
    "#\n",
    "#     names = ['svc', 'nb', 'dt', 'mlp', 'knn', 'rf', 'sgd']\n",
    "#     models = [BaggingClassifier(SVC()),\n",
    "#               BaggingClassifier(GaussianNB()),\n",
    "#               BaggingClassifier(DecisionTreeClassifier()),\n",
    "#               BaggingClassifier(MLPClassifier()),\n",
    "#               BaggingClassifier(KNeighborsClassifier()),\n",
    "#               BaggingClassifier(RandomForestClassifier()),\n",
    "#               BaggingClassifier(SGDClassifier()),]\n",
    "#\n",
    "#     # 'shuffle' is added, bc 'random_state' is set\n",
    "#     skf = StratifiedKFold(n_splits=10, random_state=1337, shuffle=True)\n",
    "#     X = train_df.drop('Class', axis=1)\n",
    "#     y = train_df[\"Class\"]\n",
    "#\n",
    "#     # res = []\n",
    "#     for i in range(len(models)):\n",
    "#         fold_num=0\n",
    "#         for train_index, test_index in skf.split(X, y):\n",
    "#             fold_num+=1\n",
    "#             X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "#             y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "#             models[i].fit(X_train, y_train)\n",
    "#             preds = models[i].predict(X_test)\n",
    "#             # print(y_test[y_test[]])\n",
    "#             # res.append(calc_metrics(y_test.tolist(), preds.tolist(), names[i], pathway_name, 'test_df', fold_num))\n",
    "#             res.append(calc_metrics(y_test.tolist(), preds.tolist(), names[i], pathway_name, 'train_df', fold_num))\n",
    "#\n",
    "# res_df = pd.DataFrame(res, columns=[\"model_name\", \"pw_name\", \"dataset_name\", \"fold_num\", \"acc\", \"f1\", \"mcc\", \"tp\", \"fp\", \"tn\", \"fn\", \"recall\", \"precision\", \"specifity\", \"fpr\", \"tpr\", \"fnr\", \"fdr\"])\n",
    "# # res_dict[pathway_name] = res_df\n",
    "#\n",
    "# file_name = '10fold_results.csv'\n",
    "# res_df.to_csv(file_name, index=False)\n",
    "# print(file_name, \"is done.\")\n",
    "#\n",
    "# del y_test, y_train, X_test, X_train, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The best classifiers for each metabolic pathway were ranked using the Kruskal-Wallis test.\n",
    "# Bu test algoritmaları nasıl sıralıyor anlamadım, benim anladığım fark var mı diye bakıyor dağılımlarda\n",
    "\n",
    "\n",
    "# res_df = pd.read_csv('10fold_results.csv')\n",
    "# res_rows = res_df.drop('fold_num', axis=1).groupby('model_name').mean()\n",
    "# for i in range(len(res_rows.index)):\n",
    "#     for j in range(i+1, len(res_rows.index)):\n",
    "#         # print(pw_name, \"-\", str(res_rows.index[i]), \"-\", str(res_rows.index[j]))\n",
    "#         stat, p = kruskal(res_rows.loc[str(res_rows.index[i])], res_rows.loc[str(res_rows.index[j])])\n",
    "#         print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "#         if p < 0.05:\n",
    "#             print(\"statistically significant p value is found: \", str(res_rows.index[i], str(res_rows.index[j])))\n",
    "#         print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# The best classifiers were pre-selected, seeking a low FP & high F-measure and CCI\n",
    "\n",
    "# best_models = {}    # {\"pathway\": [best 3 models]}\n",
    "# models = []\n",
    "# scores = {}\n",
    "# res_df[\"score\"] =  res_df[\"acc\"] + res_df[\"f1\"] - res_df[\"fp\"]\n",
    "# best_df = res_df.groupby(['pw_name','model_name'], as_index=False).mean()\n",
    "# best_df[\"score\"].sort_values()\n",
    "# for pw in pathway_names:\n",
    "#     best_models[pw] = best_df[best_df[\"pw_name\"] == pw].iloc[:3][\"model_name\"].tolist()\n",
    "#     break\n",
    "# print(best_models)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# estimators for bagging classifier\n",
    "# bagging_cls = {'svc': ('svc', BaggingClassifier(SVC())),\n",
    "#                'dt': ('dt', BaggingClassifier(DecisionTreeClassifier())),\n",
    "#                'mlp': ('mlp', BaggingClassifier(MLPClassifier())),\n",
    "#                'nb' : ('nb', BaggingClassifier(GaussianNB())),\n",
    "#                'rf': ('rf', BaggingClassifier(RandomForestClassifier())),\n",
    "#                'knn': ('knn', BaggingClassifier(KNeighborsClassifier())),\n",
    "#                'sgd': ('sgd', BaggingClassifier(SGDClassifier()))}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "params_rf = {'rf__base_estimator__bootstrap': [True],\n",
    "             'rf__base_estimator__max_depth': [80, 100]}\n",
    "# 'rf__base_estimator__max_features': [2, 3]\n",
    "# 'rf__base_estimator__min_samples_leaf': [3, 4],\n",
    "# 'rf__base_estimator__min_samples_split': [8, 10],\n",
    "# 'rf__base_estimator__n_estimators': [100, 200] \n",
    "\n",
    "params_SVC={'svc__base_estimator__C': [1, 100, 1000],\n",
    "            'svc__base_estimator__gamma': [1, 0.01, 0.0001] }\n",
    "# 'svc__base_estimator__kernel': ['rbf']\n",
    "\n",
    "params_MLP = {'mlp__base_estimator__hidden_layer_sizes': [(10,30,10),(20,)],\n",
    "              'mlp__base_estimator__activation': ['tanh', 'relu'] }\n",
    "# 'mlp__base_estimator__solver': ['sgd', 'adam'],\n",
    "# 'mlp__base_estimator__alpha': [0.0001, 0.05],\n",
    "# 'mlp__base_estimator__learning_rate': ['constant','adaptive']\n",
    "\n",
    "params_DT = { 'dt__base_estimator__max_depth': list(range(1,10)),\n",
    "              'dt__base_estimator__min_samples_leaf': list(range(1,10,2))}\n",
    "\n",
    "params_KNN = { 'knn__base_estimator__n_neighbors': list(range(1, 31)) }\n",
    "\n",
    "params_SGD = {\n",
    "    'sgd__base_estimator__penalty': ['l2'],\n",
    "    'sgd__base_estimator__n_jobs': [-1] }\n",
    "# 'sgd__base_estimator__C': [1e-7, 1e-5, 1e-3]\n",
    "\n",
    "all_params = {\"rf\": params_rf, \n",
    "              \"svc\": params_SVC, \n",
    "              \"mlp\": params_MLP, \n",
    "              \"dt\": params_DT, \n",
    "              \"knn\": params_KNN, \n",
    "              \"sgd\": params_SGD}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# voting_models_w_params = {}    # [\"pathway\": (voting model, parameters)]\n",
    "#\n",
    "# for pw_name, models in best_models.items():\n",
    "#\n",
    "#     estimators = []\n",
    "#     parameters = {}\n",
    "#\n",
    "#     for model in models:\n",
    "#         # model's estimator\n",
    "#         estimators.append(bagging_cls[model])\n",
    "#\n",
    "#         # there is no param for NB\n",
    "#         if model in all_params.keys():\n",
    "#             for param, values in all_params[model].items():\n",
    "#                 parameters[param] = values\n",
    "#\n",
    "#     # pramameters =\n",
    "#\n",
    "#     voting_model = VotingClassifier(estimators=estimators, voting='soft')\n",
    "#     voting_models_w_params[pw_name] = (voting_model, parameters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# import json\n",
    "#\n",
    "# # parameters tuning of combined algorithms\n",
    "#\n",
    "# best_scores = {}    # \"model\": best scores\n",
    "# best_params = {}    # \"model\": best parameters\n",
    "# params_rows = []\n",
    "# for pathway_name in pathway_names:\n",
    "#     df = pd.read_csv('Datasets/' + pathway_name+'_train.csv')\n",
    "#\n",
    "#     voting_model = voting_models_w_params[pathway_name][0]\n",
    "#     parameters = voting_models_w_params[pathway_name][1]\n",
    "#\n",
    "#     print(\"Starting for\", pathway_name)\n",
    "#     clf = GridSearchCV(voting_model, parameters, n_jobs=-1, verbose=3, scoring='f1_macro', cv=2)\n",
    "#     clf.fit(df.drop('Class', axis=1), df[\"Class\"])\n",
    "#\n",
    "#     best_scores[pathway_name] = clf.best_score_\n",
    "#     best_params[pathway_name] = clf.best_params_\n",
    "#     print(clf.best_score_)\n",
    "#     print(clf.best_params_)\n",
    "#\n",
    "# with open('best_params.json', 'w') as fp:\n",
    "#     json.dump(best_params, fp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# for (model, params) in voting_models_w_params.values():\n",
    "#     print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# voting_models_w_tuned_params = voting_models_w_params.copy()\n",
    "#\n",
    "# for pw, (model, params) in voting_models_w_params.items():\n",
    "#\n",
    "#     voting_models_w_params[pw][0] = model.set_params(**best_params[pw])\n",
    "#     voting_models_w_params[pw][1] = best_params[pw]\n",
    "\n",
    "# best_models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best classifiers were pre-selected, seeking a low FP & high F-measure and CCI\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "best_models = {}    # {\"pathway\": [best 3 models]}\n",
    "\n",
    "for pw_name, df in res_dict.items():\n",
    "    df = df.drop('fold_num', axis=1).groupby('model_name').mean()\n",
    "    \n",
    "    models = []\n",
    "    scores = {}\n",
    "    for i in range(len(df)):\n",
    "        model = str(res_rows.index[i])\n",
    "        row = df.loc[model]\n",
    "\n",
    "        # low FP\n",
    "        FP = row[\"fp\"]\n",
    "        \n",
    "        # high f-measure\n",
    "        F1 = row[\"f1\"]\n",
    "        \n",
    "        # high percentage of correctly classified instances\n",
    "        CCI = row[\"acc\"]\n",
    "\n",
    "        score = (CCI + F1) - FP\n",
    "        scores[model] = score\n",
    "        \n",
    "    scores = OrderedDict(sorted(scores.items(), key=lambda kv: kv[1], reverse=True))\n",
    "    \n",
    "    i = 0\n",
    "    for model, score in scores.items():\n",
    "        if (i == 3):\n",
    "            break\n",
    "        \n",
    "        models.append(model)\n",
    "        i += 1\n",
    "    \n",
    "    best_models[pw_name] = models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('rf', 1.66660625964676),\n",
       "             ('svc', 1.5911357528288048),\n",
       "             ('dt', 0.08584549131292851),\n",
       "             ('knn', -0.21657021269860977),\n",
       "             ('mlp', -0.4228980122630448),\n",
       "             ('sgd', -0.8484098342774564),\n",
       "             ('nb', -290.0267013095288)])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'amine_and_polyamine': ['knn', 'svc', 'rf'],\n",
       " 'amino_acid': ['rf', 'svc', 'dt']}"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimators for bagging classifier\n",
    "bagging_cls = {'svc': ('svc', BaggingClassifier(SVC())),\n",
    "               'dt': ('dt', BaggingClassifier(DecisionTreeClassifier())),\n",
    "               'mlp': ('mlp', BaggingClassifier(MLPClassifier())),\n",
    "               'nb' : ('nb', BaggingClassifier(GaussianNB())),\n",
    "               'rf': ('rf', BaggingClassifier(RandomForestClassifier())),\n",
    "               'knn': ('knn', BaggingClassifier(KNeighborsClassifier())),\n",
    "               'sgd': ('sgd', BaggingClassifier(SGDClassifier()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_rf = {'rf__base_estimator__bootstrap': [True],\n",
    "             'rf__base_estimator__max_depth': [80, 100]}\n",
    "# 'rf__base_estimator__max_features': [2, 3]\n",
    "# 'rf__base_estimator__min_samples_leaf': [3, 4],\n",
    "# 'rf__base_estimator__min_samples_split': [8, 10],\n",
    "# 'rf__base_estimator__n_estimators': [100, 200] \n",
    "\n",
    "params_SVC={'svc__base_estimator__C': [1, 100, 1000],\n",
    "            'svc__base_estimator__gamma': [1, 0.01, 0.0001] }\n",
    "# 'svc__base_estimator__kernel': ['rbf']\n",
    "\n",
    "params_MLP = {'mlp__base_estimator__hidden_layer_sizes': [(10,30,10),(20,)],\n",
    "              'mlp__base_estimator__activation': ['tanh', 'relu'] }\n",
    "# 'mlp__base_estimator__solver': ['sgd', 'adam'],\n",
    "# 'mlp__base_estimator__alpha': [0.0001, 0.05],\n",
    "# 'mlp__base_estimator__learning_rate': ['constant','adaptive']\n",
    "\n",
    "params_DT = { 'dt__base_estimator__max_depth': list(range(1,10)),\n",
    "              'dt__base_estimator__min_samples_leaf': list(range(1,10,2))}\n",
    "\n",
    "params_KNN = { 'knn__base_estimator__n_neighbors': list(range(1, 31)) }\n",
    "\n",
    "params_SGD = {\n",
    "    'sgd__base_estimator__penalty': ['l2'],\n",
    "    'sgd__base_estimator__n_jobs': [-1] }\n",
    "# 'sgd__base_estimator__C': [1e-7, 1e-5, 1e-3]\n",
    "\n",
    "all_params = {\"rf\": params_rf, \n",
    "              \"svc\": params_SVC, \n",
    "              \"mlp\": params_MLP, \n",
    "              \"dt\": params_DT, \n",
    "              \"knn\": params_KNN, \n",
    "              \"sgd\": params_SGD}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_models_w_params = {}    # [\"pathway\": (voting model, parameters)]\n",
    "\n",
    "for pw_name, models in best_models.items():\n",
    "    \n",
    "    estimators = []\n",
    "    parameters = {}\n",
    "    \n",
    "    for model in models:\n",
    "        # model's estimator\n",
    "        estimators.append(bagging_cls[model])\n",
    "        \n",
    "        # there is no param for NB\n",
    "        if model in all_params.keys():\n",
    "            for param, values in all_params[model].items():\n",
    "                parameters[param] = values\n",
    "                \n",
    "    # pramameters = \n",
    "        \n",
    "    voting_model = VotingClassifier(estimators=estimators, voting='soft')\n",
    "    voting_models_w_params[pw_name] = (voting_model, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting for amine_and_polyamine\n",
      "Fitting 2 folds for each of 180 candidates, totalling 360 fits\n",
      "0.9368153506734713\n",
      "{'dt__base_estimator__max_depth': 5, 'dt__base_estimator__min_samples_leaf': 9, 'mlp__base_estimator__activation': 'relu', 'mlp__base_estimator__hidden_layer_sizes': (20,)}\n",
      "Starting for amino_acid\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6855790501839225\n",
      "{'mlp__base_estimator__activation': 'tanh', 'mlp__base_estimator__hidden_layer_sizes': (20,), 'sgd__base_estimator__n_jobs': -1, 'sgd__base_estimator__penalty': 'l2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# parameters tuning of combined algorithms\n",
    "\n",
    "best_scores = {}    # \"model\": best scores\n",
    "best_params = {}    # \"model\": best parameters\n",
    "\n",
    "for pathway_name in pathway_names:\n",
    "    df = pd.read_csv('Datasets/' + pathway_name+'_train.csv')\n",
    "    \n",
    "    voting_model = voting_models_w_params[pathway_name][0]\n",
    "    parameters = voting_models_w_params[pathway_name][1]\n",
    "    \n",
    "    print(\"Starting for\", pathway_name)\n",
    "    clf = GridSearchCV(voting_model, parameters, n_jobs=-1, verbose=3, scoring='f1_macro', cv=2)\n",
    "    clf.fit(df.drop('Class', axis=1), df[\"Class\"])\n",
    "    \n",
    "    best_scores[pathway_name] = clf.best_score_\n",
    "    best_params[pathway_name] = clf.best_params_\n",
    "\n",
    "    print(clf.best_score_)\n",
    "    print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'amine_and_polyamine': 0.9368153506734713, 'amino_acid': 0.6855790501839225}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'amine_and_polyamine': {'dt__base_estimator__max_depth': 5,\n",
       "  'dt__base_estimator__min_samples_leaf': 9,\n",
       "  'mlp__base_estimator__activation': 'relu',\n",
       "  'mlp__base_estimator__hidden_layer_sizes': (20,)},\n",
       " 'amino_acid': {'mlp__base_estimator__activation': 'tanh',\n",
       "  'mlp__base_estimator__hidden_layer_sizes': (20,),\n",
       "  'sgd__base_estimator__n_jobs': -1,\n",
       "  'sgd__base_estimator__penalty': 'l2'}}"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-41771fba",
   "language": "python",
   "display_name": "PyCharm (MLBIOINFINAL)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}